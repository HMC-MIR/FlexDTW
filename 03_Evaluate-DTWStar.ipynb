{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we evaluate the accuracy of the predicted alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r DATA_ROOT\n",
    "ANNOTATIONS_ROOT = DATA_ROOT / Path('Chopin_Mazurkas/annotations_beat')\n",
    "query_list = Path('cfg_files/query.toy.list')\n",
    "# TODO: why was this test whereas the clean dir was train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate hypothesis directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First evaluate a single hypothesis directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dir(hypdir, querylist, hop_sec, savefile = None):\n",
    "    \n",
    "    allErrs = {}\n",
    "    cnt = 0\n",
    "    print(f'Processing {hypdir} ', end='')\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            assert len(parts) == 2\n",
    "            basename = os.path.basename(parts[0]) + '__' + os.path.basename(parts[1])\n",
    "            # e.g. Chopin_Op017No4_Afanassiev-2001_pid9130-01__Chopin_Op017No4_Ashkenazy-1981_pid9058-13\n",
    "            hypfile = hypdir + '/' + basename + '.pkl'\n",
    "            if not os.path.exists(hypfile):\n",
    "                print(\"X\", end='')\n",
    "                continue\n",
    "            allErrs[basename] = eval_file(hypfile, hop_sec)\n",
    "            \n",
    "            # progess bar\n",
    "            cnt += 1\n",
    "            if cnt % 500 == 0:\n",
    "                print(\".\", end='')\n",
    "    print(' done')\n",
    "    \n",
    "    if savefile:\n",
    "        with open(savefile, 'wb') as f:\n",
    "            pickle.dump(allErrs, f)\n",
    "        \n",
    "    return allErrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_file(hypfile, hop_sec):\n",
    "    '''\n",
    "    returns: array of length (# beats in piece) whose elements are the difference\n",
    "      at each beat between its predicted and ground truth locations \n",
    "    '''\n",
    "    parts = os.path.basename(hypfile).split('__')\n",
    "    assert len(parts) == 2\n",
    "    piece = extractPieceName(parts[0])\n",
    "    annotfile1 = (ANNOTATIONS_ROOT / piece / parts[0]).with_suffix('.beat')\n",
    "    annotfile2 = (ANNOTATIONS_ROOT / piece / parts[1]).with_suffix('.beat')\n",
    "    gt1 = getTimestamps(annotfile1)\n",
    "    gt2 = getTimestamps(annotfile2)\n",
    "    hypalign = loadAlignment(hypfile) # warping path in frames\n",
    "    if hypalign is None:\n",
    "        err = [] # no valid path\n",
    "    else:\n",
    "        pred2 = np.interp(gt1, hypalign[0,:]*hop_sec, hypalign[1,:]*hop_sec)\n",
    "        # piecewise linear interpolation\n",
    "        err = pred2 - gt2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPieceName(fullpath):\n",
    "    basename = os.path.basename(fullpath) # e.g. Chopin_Op068No3_Sztompka-1959_pid9170b-21\n",
    "    parts = basename.split('_')\n",
    "    piece = '_'.join(parts[0:2]) # e.g. Chopin_Op068No3\n",
    "    return piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimestamps(annotfile):\n",
    "    df = pd.read_csv(annotfile, header=None, sep='\\s+', skiprows=3)\n",
    "    return np.array(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAlignment(hypfile):\n",
    "    with open(hypfile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a single hypothesis directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypdir = 'experiments_test/dtw_clean'\n",
    "savefile = 'evaluations_test/dtw_clean.pkl'\n",
    "hop_sec = 512 * 1 / 22050\n",
    "allErrs = eval_dir(hypdir, query_list, hop_sec, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all hypothesis directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_dirs(rootdir, querylist, hop_sec, outdir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    for hypdir in glob.glob(f'{rootdir}/ssdtw_*'):\n",
    "        savefile = outdir + '/' + os.path.basename(hypdir) + '.pkl'\n",
    "        _ = eval_dir(hypdir, querylist, hop_sec, savefile = savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_ROOT = 'experiments_test'\n",
    "hop_sec = 512 * 1 / 22050\n",
    "outdir = 'evaluations_test'\n",
    "eval_all_dirs(EXPERIMENTS_ROOT, query_list, hop_sec, outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot error vs tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates(errFile, maxTol):\n",
    "    \n",
    "    # read from file\n",
    "    with open(errFile, 'rb') as f:\n",
    "        allErrs = pickle.load(f)\n",
    "    \n",
    "    # collect all errors\n",
    "    errsFlat = []\n",
    "    for query in allErrs:\n",
    "        errs = np.array(allErrs[query])\n",
    "        errsFlat.append(errs)\n",
    "    errsFlat = np.concatenate(errsFlat)\n",
    "    \n",
    "    # calculate error rates\n",
    "    errRates = np.zeros(maxTol+1)\n",
    "    for i in range(maxTol+1):\n",
    "        errRates[i] = np.mean(np.abs(errsFlat) > i/1000)\n",
    "    \n",
    "    return errRates, errsFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates_batch(indir, basenames, maxTol):\n",
    "    errRates = np.zeros((len(basenames), maxTol+1))\n",
    "    allErrVals = []\n",
    "    print('Computing error rates ', end='')\n",
    "    for i, basename in enumerate(basenames):\n",
    "        errFile = indir + '/' + basename + '.pkl'\n",
    "        errRates[i,:], errors = calc_error_rates(errFile, maxTol)\n",
    "        allErrVals.append(errors)\n",
    "        print('.', end='')\n",
    "    print(' done')\n",
    "    return errRates, allErrVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_roc(errRates, basenames):\n",
    "    numSystems = errRates.shape[0]\n",
    "    maxTol = errRates.shape[1] - 1\n",
    "    for i in range(numSystems):\n",
    "        plt.plot(np.arange(maxTol+1), errRates[i,:] * 100.0)\n",
    "    plt.legend(basenames, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ROOT_DIR = 'evaluations_test'\n",
    "toPlot = ['dtw_clean']\n",
    "maxTol = 1000 # in msec\n",
    "errRates, errVals = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot, maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_roc(errRates, toPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of selected error rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_histogram1(errRates, basenames, tols):  \n",
    "    # Histogram grouped by tolerance\n",
    "    \n",
    "    # first construct DataFrame\n",
    "    data = []\n",
    "    for i, system in enumerate(basenames):\n",
    "        for tol in tols:\n",
    "            data.append((system, tol, errRates[i,tol] * 100.0))\n",
    "    df = pd.DataFrame(data, columns = ['System', 'Tolerance', 'Error'])\n",
    "    \n",
    "    # grouped barplot\n",
    "    sns.barplot(x=\"Tolerance\", y=\"Error\", hue=\"System\", data=df)\n",
    "    plt.xlabel(\"Tolerance (ms)\", size=14)\n",
    "    plt.ylabel(\"Error Rate\", size=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = [10, 20, 50, 100, 200, 500] # in msec\n",
    "plot_grouped_histogram1(errRates, toPlot, tols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results plot for paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is touched up to look nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_histogram1(errRates_bars, errRates_dots, basenames, tols, savefile = None):  \n",
    "    # Histogram grouped by tolerance\n",
    "    \n",
    "    # first construct DataFrame\n",
    "    data = []\n",
    "    for i, system in enumerate(basenames):\n",
    "        for tol in tols:\n",
    "            data.append((system, tol, errRates[i,tol] * 100.0))\n",
    "    df = pd.DataFrame(data, columns = ['System', 'Tolerance', 'Error'])\n",
    "    \n",
    "    # grouped barplot (DTW & WSDTW)\n",
    "    sns.barplot(x=\"Tolerance\", y=\"Error\", hue=\"System\", data=df)\n",
    "    plt.xlabel(\"Tolerance (ms)\", size=14)\n",
    "    plt.ylabel(\"Error Rate (%)\", size=14)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # overlay dots for SSDTW results\n",
    "    width_bar = .135\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for i, tol in enumerate(tols):\n",
    "        for j in range(errRates_dots.shape[0]):\n",
    "            x_coords.append(i+(-1.5+j)*width_bar)\n",
    "            y_coords.append(errRates_dots[j,tol] * 100.0)\n",
    "    plt.plot(x_coords, y_coords, 'ko', markersize=3)\n",
    "    \n",
    "    if savefile:\n",
    "        plt.savefig(savefile, bbox_inches = 'tight')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = [10, 20, 50, 100, 200, 500] # in msec\n",
    "display_names = ['DTW']\n",
    "savefile = 'results.png'\n",
    "plot_grouped_histogram1(errRates[0:6,:], errRates[6:,:], display_names, tols, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_runtimes, dtw_sizes = pickle.load(open('dtw_prof.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers for runtime table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_avgs = np.mean(np.sum(dtw_runtimes[::-1,:,:], axis=2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of runtime by component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DTW percent of total runtime by component\n",
    "dtw_avgs = np.mean(dtw_runtimes, axis=1)\n",
    "dtw_avgs = dtw_avgs / np.sum(dtw_avgs, axis=1, keepdims=True) * 100.0\n",
    "dtw_avgs = dtw_avgs[::-1,:]\n",
    "dtw_avgs = np.hstack((dtw_avgs, np.zeros((6,2)))) # order: cost, frm dp, frm back, seg dp, seg back\n",
    "dtw_df = pd.DataFrame(dtw_avgs, columns=['Cost', 'Frm DP', 'Frm Back','Seg DP', 'Seg Back'], index=dtw_sizes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "\n",
    "# DTW plot\n",
    "dtw_df.plot(kind=\"bar\", stacked=True, colormap=\"rainbow\", ax=axes)\n",
    "plt.title(\"DTW\")\n",
    "plt.ylabel(\"% Total Runtime\", size=13),\n",
    "plt.ylim(0, 100)\n",
    "plt.yticks(range(0, 101, 10))\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    ")\n",
    "ax.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('runtime.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Stats for paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting some stats on the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'Chopin_Mazurkas/wav_22050_mono'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAudioStats(indir):\n",
    "    durs = []\n",
    "    for infile in glob.glob(f'{indir}/*.wav'):\n",
    "        y, sr = lb.load(infile)\n",
    "        durs.append(len(y)/sr)\n",
    "    durs = np.array(durs)\n",
    "    \n",
    "    print(os.path.basename(indir))\n",
    "    print('---------')\n",
    "    print(f'Min: {np.min(durs)} s')\n",
    "    print(f'Max: {np.max(durs)} s')\n",
    "    print(f'Mean: {np.mean(durs)} s')\n",
    "    print(f'Std: {np.std(durs)} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indir in glob.glob(f'{root_dir}/*'):\n",
    "    printAudioStats(indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
